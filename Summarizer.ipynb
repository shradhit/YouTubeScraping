{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the packages\n",
    "\n",
    "import youtube_dl\n",
    "import webvtt\n",
    "import re\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import cosine_distance\n",
    "\n",
    "import heapq\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a url link:\n",
      "\n",
      "The link is:  https://www.youtube.com/watch?v=1jUmohFv_zw\n",
      "Done, proceed further\n"
     ]
    }
   ],
   "source": [
    "# Please enter a link\n",
    "link = input(\"Please enter a url link:\\n\")\n",
    "if not link:\n",
    "    link = \"https://www.youtube.com/watch?v=1jUmohFv_zw\"\n",
    "print(\"The link is: \", link)\n",
    "print(\"Done, proceed further\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting audio and subtitles as text!\n",
      "Please proceed with extracting the below video contents!\n",
      "\n",
      " 'uploader','uploader_url','upload_date','creator','title','description','categories',\n",
      "      'duration','view_count', 'like_count', 'dislike_count','average_rating','start_time', 'end_time',\n",
      "      'release_date', 'release_year'\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\om20817\\Documents\\webscraping_sentiment_analysis\")\n",
    "\n",
    "# To avoid any error while parsing the video, we have created the MyLogger class\n",
    "class MyLogger(object):\n",
    "    def debug(self, msg):\n",
    "        pass\n",
    "\n",
    "    def warning(self, msg):\n",
    "        pass\n",
    "\n",
    "    def error(self, msg):\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# Dictionary to accept some inputs\n",
    "ydl_opts = {\n",
    "    \"writesubtitles\": True,\n",
    "    \"writeautomaticsub\": True,\n",
    "    \"writeinfojson\": True,\n",
    "    \"format\": \"bestaudio/best\",\n",
    "    \"logger\": MyLogger(),\n",
    "    \"keepvideo\": False,\n",
    "    \"postprocessors\": [\n",
    "        {\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"wav\",\n",
    "            \"preferredquality\": \"192\",\n",
    "        }\n",
    "    ],\n",
    "    \"postprocessor_args\": [\"-ar\", \"16000\"],\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Ignore below 2 lines\n",
    "# with youtube_dl.YoutubeDL(youtube_dl_options) as youtube_dl_client:\n",
    "#    youtube_dl_client.download([link])\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Extract the audio from the video link along with the subtitles as text\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    meta = ydl.extract_info(link, download=True)\n",
    "\n",
    "print(\"Finished extracting audio and subtitles as text!\")\n",
    "print(\"Please proceed with extracting the below video contents!\")\n",
    "print()\n",
    "print(\n",
    "    \"\"\" 'uploader','uploader_url','upload_date','creator','title','description','categories',\n",
    "      'duration','view_count', 'like_count', 'dislike_count','average_rating','start_time', 'end_time',\n",
    "      'release_date', 'release_year'\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"uploader\",\n",
    "    \"uploader_url\",\n",
    "    \"upload_date\",\n",
    "    \"creator\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"categories\",\n",
    "    \"duration\",\n",
    "    \"view_count\",\n",
    "    \"like_count\",\n",
    "    \"dislike_count\",\n",
    "    \"average_rating\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"release_date\",\n",
    "    \"release_year\",\n",
    "]\n",
    "\n",
    "filtered_d = dict((k, meta[k]) for k in keys if k in meta)\n",
    "df = pd.DataFrame.from_dict(filtered_d, orient=\"index\").T\n",
    "df.index = df[\"title\"]\n",
    "\n",
    "sub_titles = glob.glob('./*.en.vtt')\n",
    "sub_titles[0]\n",
    "\n",
    "vtt = webvtt.read(sub_titles[0])\n",
    "\n",
    "# Store the starting and ending point of each sentence as part of the start and end list\n",
    "start_list = list()\n",
    "end_list = list()\n",
    "\n",
    "# Storing all the lines as part of the lines list\n",
    "lines = []\n",
    "\n",
    "for x in range(len(vtt)):\n",
    "    start_list.append(vtt[x].start)\n",
    "    end_list.append(vtt[x].end)\n",
    "\n",
    "for line in vtt:\n",
    "    lines.append(line.text.strip().splitlines())\n",
    "\n",
    "lines = [\" \".join(item) for item in lines]\n",
    "\n",
    "final_df = pd.DataFrame({'Start_time':start_list,'End_time':end_list,'Statement':lines})\n",
    "final_df['Statement'] = [w.replace('&gt;&gt;', '') for w in final_df['Statement']]\n",
    "final_df['Statement'] = [w.replace(' &gt;&gt;', '') for w in final_df['Statement']]\n",
    "final_df['Statement'] = [w.replace('&gt;&gt; Reporter:', '') for w in final_df['Statement']]\n",
    "\n",
    "# Initialize the vader model\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute sentiment scores and labels\n",
    "sentiment_scores_vader = [\n",
    "    sid_obj.polarity_scores(article) for article in final_df.Statement\n",
    "]\n",
    "\n",
    "sentiment_category_positive = []\n",
    "sentiment_category_neutral = []\n",
    "sentiment_category_negative = []\n",
    "sentiment_category_compound = []\n",
    "\n",
    "for sentiments in sentiment_scores_vader:\n",
    "    sentiment_category_positive.append(sentiments[\"pos\"])\n",
    "    sentiment_category_neutral.append(sentiments[\"neu\"])\n",
    "    sentiment_category_negative.append(sentiments[\"neg\"])\n",
    "    sentiment_category_compound.append(sentiments[\"compound\"])\n",
    "\n",
    "\n",
    "# Sentiment statistics per statement\n",
    "sentiment_df = pd.DataFrame(\n",
    "    [\n",
    "        [article for article in final_df.Statement],\n",
    "        sentiment_category_positive,\n",
    "        sentiment_category_neutral,\n",
    "        sentiment_category_negative,\n",
    "        sentiment_category_compound,\n",
    "    ]\n",
    ").T\n",
    "\n",
    "sentiment_df[\"Start_time\"] = start_list\n",
    "sentiment_df[\"End_time\"] = end_list\n",
    "\n",
    "\n",
    "sentiment_df.columns = [\n",
    "    \"Statement\",\n",
    "    \"positive_polarity\",\n",
    "    \"neutral_polarity\",\n",
    "    \"negative_polarity\",\n",
    "    \"overall_polarity\",\n",
    "    \"Start_time\",\n",
    "    \"End_time\",\n",
    "]\n",
    "\n",
    "\n",
    "sentiment_df[\"Sentiment\"] = [\n",
    "    \"Positive\" if w > 0 else \"Negative\" if w < 0 else \"Neutral\"\n",
    "    for w in sentiment_df[\"overall_polarity\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - NLTK word frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LUCKILY FOR ME, IT’S JUST A SHORT DELAY FORLUCKILY FOR ME, IT’S JUST A SHORT DELAY FOR NOW BUT THAT’S NOT THEJUST A SHORT DELAY FOR NOW BUT THAT’S NOT THE CASE FOR SO MANYNOW BUT THAT’S NOT THE CASE FOR SO MANY OTHERS.CASE FOR SO MANY OTHERS. Reporter: HOSPITALS DECIDING MANY CLINICAL Reporter: HOSPITALS DECIDING MANY CLINICAL TRIALS COULD EXPOSEDECIDING MANY CLINICAL TRIALS COULD EXPOSE VULNERABLE PATIENTSTRIALS COULD EXPOSE VULNERABLE PATIENTS AND HEALTH CAREVULNERABLE PATIENTS AND HEALTH CARE WORKERS TO COVID-19AND HEALTH CARE WORKERS TO COVID-19 BUT ANNA’S DOCTORWORKERS TO COVID-19 BUT ANNA’S DOCTOR WORRIES SHE DOESN’TBUT ANNA’S DOCTOR WORRIES SHE DOESN’T HAVE TIME TO SPARE.WORRIES SHE DOESN’T HAVE TIME TO SPARE.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []  # Empty list to store all the sentences\n",
    "\n",
    "for sentence in sentiment_df[\"Statement\"]:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# Creating the article or corpus to perform summarization\n",
    "article_text = \"\".join(sentences)\n",
    "\n",
    "# Pre-processing\n",
    "# Removing square brackets and extra spaces\n",
    "article_text = re.sub(r\"\\[[0-9]*\\]\", \" \", article_text)\n",
    "article_text = re.sub(r\"\\s+\", \" \", article_text)\n",
    "\n",
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub(r\"[^a-zA-Z]\", \" \", article_text)\n",
    "formatted_article_text = re.sub(r\"\\s+\", \" \", formatted_article_text)\n",
    "\n",
    "# Use the formatted article text to create weighted frequency for the words and replace these weighted frequencies with\n",
    "# the words in the article text\n",
    "\n",
    "# Convert text to sentences\n",
    "sentence_list = nltk.sent_tokenize(article_text)\n",
    "\n",
    "# Remove all the stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append(('&gt;&gt;','&gt','&gt','&gt;','Reporter'))\n",
    "\n",
    "# Find the weighted frequency of all the words\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "# Lower case all the words i.e. keys in the word frequencies dictionary\n",
    "word_frequencies = {k.lower(): v for k, v in word_frequencies.items()}\n",
    "\n",
    "# Calculate the maxium word frequency\n",
    "maximum_frequency = max(word_frequencies.values())\n",
    "\n",
    "# Calculate word frequency for each word\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
    "\n",
    "# Calculate the overall score of the sentence\n",
    "sentence_scores = {}  # key: sentences, values: scores of the sentences\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(\" \")) < 10000:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "# Generating the final summary\n",
    "n = 2  # The number of sentences to generate in the summary\n",
    "summary_sentences = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)\n",
    "summary = \" \".join(summary_sentences)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Cosine Similarity and page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A F T E R   L E A R N I N G   H E R   L A S T   H O P E   T O   B E A T   S T A G E   F O U R   C O L O N',\n",
       " 'T H A T   T R E A T M E N T   N O W .     R e p o r t e r :   H O S P I T A L S   A R E   A S K I N G   M A N Y   T O   D O',\n",
       " 'L A S T   H O P E   T O   B E A T   S T A G E   F O U R   C O L O N   C A N C E R   I S   B E I N G',\n",
       " 'A N D   F O R   T H O S E   P E O P L E   T O   B E   S O   I R R E S P O N S I B L E   A N D   T O   D R I N K   I S   M O R E',\n",
       " 'B U T   A N N A ’ S   D O C T O R   W O R R I E S   S H E   D O E S N ’ T   H A V E   T I M E   T O   S P A R E .']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Sentence Similarity\n",
    "\n",
    "\n",
    "def read_article(colname='Statement'):\n",
    "    sentences = []\n",
    "    for sentence in sentiment_df[colname]:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stop_words.append((\"&gt;&gt;\", \"&gt\", \"&gt\", \"&gt;\", \"Reporter\"))\n",
    "    if stop_words is None:\n",
    "        stop_words = []\n",
    "\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # Build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "\n",
    "    # Create an empty similarity matrix of n*n sentence size in video\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                # ignore if both are same sentences\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(\n",
    "                sentences[idx1], sentences[idx2], stopwords\n",
    "            )\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# Generate similarity matrix across sentences\n",
    "sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "# Rank sentences in similarity matrix\n",
    "sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "\n",
    "# Calculate the scores\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(\n",
    "    ((scores[i], s) for i, s in enumerate(sentences)), reverse=True\n",
    ")\n",
    "\n",
    "# Empty list to save the summarize text\n",
    "summarize_text = []\n",
    "\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "summarize_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - Gensim summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word summary\n",
      "Reporter: NEARLY 2 MILLION AMERICANS ARE Reporter: NEARLY 2 MILLION AMERICANS ARE DIAGNOSED WITH CANCERMILLION AMERICANS ARE DIAGNOSED WITH CANCER EVERY YEAR.DIAGNOSED WITH CANCER EVERY YEAR.\n",
      "Reporter: HOSPITALS ARE ASKING MANY TO DO Reporter: HOSPITALS ARE ASKING MANY TO DO APPOINTMENTS ONLINE,ARE ASKING MANY TO DO APPOINTMENTS ONLINE, SKIP FOLLOWUP VISITSAPPOINTMENTS ONLINE, SKIP FOLLOWUP VISITS AND IN SOME CASESSKIP FOLLOWUP VISITS AND IN SOME CASES DELAY TREATMENTS.AND IN SOME CASES DELAY TREATMENTS.\n",
      "FOR 35-YEAR-OLD ANNA, IT’S ESPECIALLY HARDFOR 35-YEAR-OLD ANNA, IT’S ESPECIALLY HARD SEEING OTHER YOUNGIT’S ESPECIALLY HARD SEEING OTHER YOUNG PEOPLE IGNORING THESEEING OTHER YOUNG PEOPLE IGNORING THE GUIDANCE ON SOCIALPEOPLE IGNORING THE GUIDANCE ON SOCIAL DISTANCING.GUIDANCE ON SOCIAL DISTANCING.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in sentiment_df['Statement']:\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "paragraph = ''.join(sentences)\n",
    "\n",
    "\n",
    "# Method a\n",
    "# Summary of 5% of the original content\n",
    "summ_per = summarize(paragraph,ratio=0.5)\n",
    "#print(\"Percent Summary\")\n",
    "#print(summ_per)\n",
    "\n",
    "# Method b\n",
    "# Summary 200 words\n",
    "summ_words = summarize(paragraph,word_count=100)\n",
    "print(\"Word summary\")\n",
    "print(summ_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
